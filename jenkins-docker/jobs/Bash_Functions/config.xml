<?xml version='1.1' encoding='UTF-8'?>
<project>
  <actions/>
  <description>Job will be triggered by Check for updates. &#xd;
&#xd;
Gives a central location to store bash functions used in the pipeline.</description>
  <keepDependencies>false</keepDependencies>
  <properties/>
  <scm class="hudson.scm.NullSCM"/>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>echo &apos;
assume_role(){
  local role_arn=${1:-&quot;arn:aws:iam::&apos;${app_acct_id}&apos;:role/&apos;${jenkins_provisioning_assume_role_name}&apos;&quot;}
  OUTPUT=$(aws sts assume-role \
      --role-arn $role_arn \
      --role-session-name &quot;teardown-rebuild&quot; \
      --query &quot;Credentials.[AccessKeyId,SecretAccessKey,SessionToken]&quot; \
      --output text)

  export AWS_ACCESS_KEY_ID=$(echo $OUTPUT | awk &quot;{print \$1}&quot;)
  export AWS_SECRET_ACCESS_KEY=$(echo $OUTPUT | awk &quot;{print \$2}&quot;)
  export AWS_SESSION_TOKEN=$(echo $OUTPUT | awk &quot;{print \$3}&quot;)
  export AWS_DEFAULT_REGION=us-east-1
}

reset_role(){
  unset AWS_ACCESS_KEY_ID
  unset AWS_SECRET_ACCESS_KEY
  unset AWS_SESSION_TOKEN
}

export -f assume_role
export -f reset_role
        &apos; &gt; role_functions.sh</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command># Provides automation for swapping instances between staging and live target groups, using green/blue deployment strategy.

# Script Usage:
# To use this script, specify the names of the staging and live target groups, as well as JSON files
# containing tag filters for identifying instances to swap between the target groups.

# Example Usage:
# swap_stacks &quot;staging-tg-name&quot; &quot;live-tg-name&quot; &quot;staging-httpd-tags.txt&quot; &quot;live-httpd-tags.txt&quot;

# Function to get Target Group ARN by name

echo &apos;
get_target_group_arn_by_name() {
  # Use AWS CLI to describe target groups and extract the ARN by name
  aws elbv2 describe-target-groups \
    --query &quot;TargetGroups[?TargetGroupName==\`$1\`].TargetGroupArn&quot; \
    --output text
}

# Function to get Target Group VPC by name
get_target_group_vpc_by_tg_name() {
  local target_group_name=$1
  # Use AWS CLI to describe the target group and extract the VPC ID
  aws elbv2 describe-target-groups \
    --names &quot;$target_group_name&quot; \
    --query &quot;TargetGroups[0].VpcId&quot; \
    --output text
}

# Function to get private IP by tags
get_private_ip_by_tags() {
  local tag_filters_file=&quot;$1&quot;
  # Use AWS CLI to describe instances with specified tag filters from a file and extract private IPs
  aws ec2 describe-instances \
    --filters &quot;file://$tag_filters_file&quot; \
    --query &quot;Reservations[].Instances[].PrivateIpAddress&quot; \
    --output text
}

# Function to check if an EC2 instance is in the same VPC as a target group
is_ec2_in_same_vpc_as_target_group() {
  local ec2_private_ip=&quot;$1&quot;
  local target_group_vpc_id=&quot;$2&quot;
  # Use AWS CLI to describe the EC2 instance and extract its VPC ID
  local ec2_vpc_id=$(aws ec2 describe-instances \
    --filters &quot;Name=private-ip-address,Values=$ec2_private_ip&quot; \
    --query &quot;Reservations[0].Instances[0].VpcId&quot; \
    --output text)

  # Compare the EC2 instances VPC ID with the known target groups VPC ID
  if [ &quot;$ec2_vpc_id&quot; == &quot;$target_group_vpc_id&quot; ]; then
    echo &quot;The EC2 instance with private IP $ec2_private_ip is in the same VPC as the target group.&quot;
    return 0  # Return success (0)
  else
    echo &quot;The EC2 instance with private IP $ec2_private_ip is NOT in the same VPC as the target group.&quot;
    return 1  # Return failure (non-zero)
  fi
}

# Function to register targets in a target group
register_targets() {
  local local_target_group_vpc=&quot;$1&quot;
  local local_target_group_arn=&quot;$2&quot;
  local httpd_priv_ips=(&quot;${@:3}&quot;)

  for local_httpd_priv_ips in &quot;${httpd_priv_ips[@]}&quot;; do
    # Check if the EC2 instance is in the same VPC as the target group and register it accordingly
    if is_ec2_in_same_vpc_as_target_group &quot;$local_httpd_priv_ips&quot; &quot;$local_target_group_vpc&quot;; then
      aws elbv2 register-targets --target-group-arn $local_target_group_arn --targets &quot;Id=$local_httpd_priv_ips&quot;
    else
      aws elbv2 register-targets --target-group-arn $local_target_group_arn --targets &quot;Id=$local_httpd_priv_ips,AvailabilityZone=all&quot;
    fi
  done
}

# Function to deregister targets from a target group
deregister_targets() {
  local local_target_group_vpc=&quot;$1&quot;
  local local_target_group_arn=&quot;$2&quot;
  local httpd_priv_ips=(&quot;${@:3}&quot;)

  for httpd_priv_ip in &quot;${httpd_priv_ips[@]}&quot;; do
    # Check if the EC2 instance is in the same VPC as the target group and deregister it accordingly
    if is_ec2_in_same_vpc_as_target_group &quot;$httpd_priv_ip&quot; &quot;$local_target_group_vpc&quot;; then
      aws elbv2 deregister-targets --target-group-arn $local_target_group_arn --targets &quot;Id=$httpd_priv_ip&quot;
    else
      aws elbv2 deregister-targets --target-group-arn $local_target_group_arn --targets &quot;Id=$httpd_priv_ip,AvailabilityZone=all&quot;
    fi
  done
}

# Function to wait for a target group to become healthy for all instances
wait_for_target_group_health() {
  local target_group_arn=&quot;$1&quot;
  local wait_for_health=&quot;$2&quot;
  local target_instance_ips=(&quot;${@:3}&quot;)

  local timeout_secs=600
  local start_time=$(date +%s)

  for target_instance_ip in &quot;${target_instance_ips[@]}&quot;; do
    local health_status=$(aws elbv2 describe-target-health \
      --target-group-arn &quot;$target_group_arn&quot; \
      --targets &quot;Id=$target_instance_ip&quot; \
      --query &quot;TargetHealthDescriptions[0].TargetHealth.State&quot; \
      --output text
    )

    while [[ ${health_status^^} != ${wait_for_health^^} ]]; do
      local elapsed_time=$(( $(date +%s) - start_time ))
      if [ $elapsed_time -ge $timeout_secs ]; then
        echo &quot;Target group health check exceeds timeout&quot;
        echo &quot;Target=$target_instance_ip Current_Health=${health_status^^} wait_for_health=$wait_for_health&quot;
        exit 1
      fi

      # Use AWS CLI to describe the target health status and update the health_status variable
      health_status=$(aws elbv2 describe-target-health \
        --target-group-arn &quot;$target_group_arn&quot; \
        --targets &quot;Id=$target_instance_ip&quot; \
        --query &quot;TargetHealthDescriptions[0].TargetHealth.State&quot; \
        --output text
      )
      echo &quot;$target_instance_ip - ${health_status^^}.  Waiting for health of ${wait_for_health^^}&quot;
      sleep 10  # Wait for 10 seconds before checking again
    done
  done
}

# Function to swap instances between staging and live target groups
swap_stacks() {
  local staging_tg_name=$1
  local live_tg_name=$2
  local staging_httpd_tags_file=&quot;$3&quot;
  local live_httpd_tags_file=&quot;$4&quot;
  local fail_empty_stacks=&quot;${5,,:-&quot;false&quot;,,}&quot;  # Set to true for stable prod and release testing
  echo &quot;staging_tg_name=$1&quot;
  echo &quot;live_tg_name=$2&quot;
  echo &quot;staging_httpd_tags_file=$3&quot;
  echo &quot;live_httpd_tags_file=$4&quot;

  # Get Target Group ARNs using the function
  local local_staging_target_group_arn=$(get_target_group_arn_by_name &quot;$staging_tg_name&quot;)
  local local_live_target_group_arn=$(get_target_group_arn_by_name &quot;$live_tg_name&quot;)

  # Get target group VPC. Both target groups are known to be in the same VPC
  local local_target_group_vpc=$(get_target_group_vpc_by_tg_name &quot;$live_tg_name&quot;)

  # Get private IPs using the function with tags from files
  local local_staging_httpd_instance_prv_ips=($(get_private_ip_by_tags &quot;$staging_httpd_tags_file&quot;))
  local local_live_httpd_instance_prv_ips=($(get_private_ip_by_tags &quot;$live_httpd_tags_file&quot;))

  # Promote Staging to Live (green to blue)
  echo &quot;Promote Staging to Live (green to blue)&quot;
  if [ -z &quot;${local_staging_httpd_instance_prv_ips}&quot; ]; then
    echo &quot;No staging IP Addresses found when promoting to live server.&quot;
    echo &quot;Ensure nodes have been created with proper tags for Node, Project, and Stack keys.&quot;
    if [ &quot;${fail_empty_stacks}&quot; = &quot;true&quot; ]; then
      echo &quot;failing build&quot;
      exit 1
    fi
  else
    register_targets &quot;$local_target_group_vpc&quot; &quot;$local_live_target_group_arn&quot; &quot;${local_staging_httpd_instance_prv_ips[@]}&quot;
    wait_for_target_group_health &quot;$local_live_target_group_arn&quot; &quot;HEALTHY&quot; &quot;${local_staging_httpd_instance_prv_ips[@]}&quot;
  fi

  # Demote Live to Staging (blue to green)
  echo &quot;Demote Live to Staging (blue to green)&quot;
  if [ -z &quot;${local_live_httpd_instance_prv_ips}&quot; ]; then
    echo &quot;No live IP Addresses found when demoting to staging server.&quot;
    echo &quot;Ensure nodes have been created with proper tags for Node, Project, and Stack keys.&quot;
    if [ &quot;${fail_empty_stacks}&quot; = &quot;true&quot; ]; then
      echo &quot;failing build&quot;
      exit 1
    fi
  else
    register_targets &quot;$local_target_group_vpc&quot; &quot;$local_staging_target_group_arn&quot; &quot;${local_live_httpd_instance_prv_ips[@]}&quot;
    wait_for_target_group_health &quot;$local_staging_target_group_arn&quot; &quot;HEALTHY&quot; &quot;${local_live_httpd_instance_prv_ips[@]}&quot;
  fi

  # Deregister previous Live from Live TG (remove blue from blue)
  echo &quot;Deregister previous Live from Live TG (remove blue from blue)&quot;
  if [ -z &quot;${local_live_httpd_instance_prv_ips}&quot; ]; then
    echo &quot;No live IP Addresses found when deregistering from live TG.&quot;
    echo &quot;Ensure nodes have been created with proper tags for Node, Project, and Stack keys and that it has a TGA.&quot;
    if [ &quot;${fail_empty_stacks}&quot; = &quot;true&quot; ]; then
      echo &quot;failing build&quot;
      exit 1
    fi
  else
    deregister_targets &quot;$local_target_group_vpc&quot; &quot;$local_live_target_group_arn&quot; &quot;${local_live_httpd_instance_prv_ips[@]}&quot;
    # Wait for draining
    wait_for_target_group_health &quot;$local_live_target_group_arn&quot; &quot;UNUSED&quot; &quot;${local_live_httpd_instance_prv_ips[@]}&quot;
  fi

  # Deregister previous Staging from Staging TG (remove green from green)
  echo &quot;Deregister previous Staging from Staging TG (remove green from green)&quot;
  if [ -z &quot;${local_staging_httpd_instance_prv_ips}&quot; ]; then
    echo &quot;No staging IP Addresses found when deregistering from live TG.&quot;
    echo &quot;Ensure nodes have been created with proper tags for Node, Project, and Stack keys and that it has a TGA.&quot;
    if [ &quot;${fail_empty_stacks}&quot; = &quot;true&quot; ]; then
      echo &quot;failing build&quot;
      exit 1
    fi
  else
    deregister_targets &quot;$local_target_group_vpc&quot; &quot;$local_staging_target_group_arn&quot; &quot;${local_staging_httpd_instance_prv_ips[@]}&quot;
    # Wait for draining
    wait_for_target_group_health &quot;$local_staging_target_group_arn&quot; &quot;UNUSED&quot; &quot;${local_staging_httpd_instance_prv_ips[@]}&quot;
  fi
  echo &quot;Swap Stacks complete&quot;
}&apos; &gt; target_group_attachment_functions.sh</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>cat &lt;&lt;&apos;EOF&apos; &gt; pic_sure_rds_snapshot.sh
# Funtion to return latest snapshot for rds instance
get_latest_rds_snapshot_id() {
  local picsure_rds_strategy=&quot;$1&quot;
  local stack=&quot;$2&quot;
  local project=&quot;$3&quot;

  if [ &quot;${picsure_rds_strategy}&quot; == &quot;MANAGED_USE_LATEST_SS&quot; ]; then
    db_instance_identifier=$(get_db_instance_identifier_by_tag &quot;$live_stack&quot; &quot;$env_project&quot;)
    latest_snapshot=$(aws rds describe-db-snapshots \
      --db-instance-identifier &quot;$db_instance_identifier&quot; \
      --query &quot;DBSnapshots | sort_by(@, &amp;SnapshotCreateTime) | [-1].DBSnapshotIdentifier&quot; \
      --output text)
  fi
  echo &quot;$latest_snapshot&quot;
}

# find db instance by stack and project tags
get_db_instance_identifier_by_tag() {
  local stack=&quot;$1&quot;
  local project=&quot;$2&quot;
  local identifier
  identifier=$(aws rds describe-db-instances | jq --arg stack &quot;$stack&quot; --arg project &quot;$project&quot; -r &apos;.DBInstances[] | select(.TagList[] | .Key == &quot;Stack&quot; and .Value == $stack) | select(.TagList[] | .Key == &quot;Project&quot; and .Value == $project) | .DBInstanceIdentifier&apos;)
  echo &quot;$identifier&quot;
}

create_rds_snapshot_by_tag() {
  local live_stack=&quot;$1&quot;
  local env_project=&quot;$2&quot;

  # Find the RDS instance based on the specified tag
  local db_instance_identifier
  db_instance_identifier=$(get_db_instance_identifier_by_tag &quot;$live_stack&quot; &quot;$env_project&quot;)

  if [ -z &quot;$db_instance_identifier&quot; ]; then
    echo &quot;No RDS instance found with tag: $live_stack and $env_project&quot;
    return 1
  fi
  local timestamp=$(date +&quot;%Y-%m-%d-%H-%M-%S&quot;)
  local snapshot_identifier=&quot;${db_instance_identifier}-snapshot-${timestamp}&quot;

  # Create a snapshot for the RDS instance
  aws rds create-db-snapshot \
    --db-instance-identifier &quot;$db_instance_identifier&quot; \
    --db-snapshot-identifier &quot;$snapshot_identifier&quot;

  while true; do
    local status=$(aws rds describe-db-snapshots \
      --db-instance-identifier &quot;$db_instance_identifier&quot; \
      --db-snapshot-identifier &quot;$snapshot_identifier&quot; \
      --query &quot;DBSnapshots[0].Status&quot; --output text)

    if [ &quot;${status,,}&quot; = &quot;available&quot; ]; then
      echo &quot;$snapshot_identifier&quot;
      return 0
    else
      sleep 20
    fi
  done
}

# Function to wait for an RDS instance to become available and then create a snapshot
create_snapshot_when_available() {
    local DB_INSTANCE_IDENTIFIER=$1
    local SNAPSHOT_IDENTIFIER=&quot;${DB_INSTANCE_IDENTIFIER}-snapshot-$(date +%Y%m%d-%H%M%S)&quot;

    while true; do
        status=$(aws rds describe-db-instances --db-instance-identifier &quot;${DB_INSTANCE_IDENTIFIER}&quot; --query &apos;DBInstances[*].DBInstanceStatus&apos; --output text)
        echo &quot;Current status: $status&quot;
        if [ &quot;$status&quot; == &quot;available&quot; ]; then
            echo &quot;Instance is available. Creating snapshot...&quot;
            aws rds create-db-snapshot --db-instance-identifier &quot;${DB_INSTANCE_IDENTIFIER}&quot; --db-snapshot-identifier &quot;${SNAPSHOT_IDENTIFIER}&quot;
            echo &quot;Snapshot ${SNAPSHOT_IDENTIFIER} creation initiated.&quot;
            break
        else
            echo &quot;Waiting for instance ${DB_INSTANCE_IDENTIFIER} to become available...&quot;
            sleep 30
        fi
    done
}

# Function to retrieve the most recent snapshot identifier for a specified RDS instance
get_latest_snapshot_identifier() {
    local DB_INSTANCE_IDENTIFIER=$1
   	local output=$(aws rds describe-db-snapshots \
    --db-instance-identifier &quot;${DB_INSTANCE_IDENTIFIER}&quot; \
    --query &apos;DBSnapshots[?SnapshotType==`manual`].[DBSnapshotIdentifier,SnapshotCreateTime] | sort_by(@, &amp;[1]) | [-1]&apos; \
    --output text)

    local snapshot_identifier=$(echo $output | awk &apos;{print $1}&apos;)
    echo &quot;The latest snapshot identifier for ${DB_INSTANCE_IDENTIFIER} is: $snapshot_identifier&quot;
    # Optionally, you can return the identifier if you need to use it in another context
    echo $snapshot_identifier
}

# Function to keep only the latest 3 manually created RDS snapshots and delete the rest
manage_rds_snapshots() {
  local db_instance_identifier=&quot;$1&quot;

  # Get the list of manually created snapshot identifiers sorted by creation time in descending order
  # Filters out automated snapshots and assumes snapshot names contain the creation date in a sortable format
  local snapshots_to_delete=$(aws rds describe-db-snapshots --db-instance-identifier &quot;$db_instance_identifier&quot; \
                               --query &apos;DBSnapshots[?SnapshotType==`manual`].[DBSnapshotIdentifier,SnapshotCreateTime]&apos; \
                               --output text | sort -k2,2r | awk &apos;NR&gt;3 {print $1}&apos;)

  # Loop through and delete snapshots older than the 3 most recent manually created ones
  for snapshot_id in $snapshots_to_delete; do
    echo &quot;Deleting snapshot: $snapshot_id&quot;
    aws rds delete-db-snapshot --db-snapshot-identifier &quot;$snapshot_id&quot;
    # Consider adding error handling here to check the success of the deletion
  done
}
EOF
</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>cat &lt;&lt;&apos;EOF&apos; &gt; jwt_token.sh
# Function to extract and return the expiration date of the JWT token
get_jwt_expiration_date() {
    local token=&quot;$1&quot;
    local payload

    # Decode payload
    payload=$(echo &quot;$token&quot; | cut -d&quot;.&quot; -f2 | tr &apos;_-&apos; &apos;/+&apos; | tr -d &apos;=&apos; | base64 -d 2&gt;/dev/null)
    # Extract and convert exp field to readable date
    local exp=$(echo &quot;$payload&quot; | jq &apos;.exp&apos; | tr -d &apos;&quot;&apos;)
    local exp_date=$(date -d &quot;@$exp&quot; 2&gt;/dev/null)

    echo &quot;$exp_date&quot;
}

# Function to check if the token is expiring within the next 30 days
is_expiring_soon() {
    local exp_date=&quot;$1&quot;
    local exp_date_in_seconds=$(date -d &quot;$exp_date&quot; +%s)
    local current_date_plus_30=$(date -d &quot;+30 days&quot; +%s)

    if [ &quot;$exp_date_in_seconds&quot; -le &quot;$current_date_plus_30&quot; ]; then
        return 0 # Token is expiring soon
    else
        return 1 # Token is not expiring soon
    fi
}

# Create JWT token
# Clones a Git repository, generates a valid token, and exports it to an environment variable
# Parameters:
#   $1: Path to the secrets file
#   $2: Git repository URL for jwt-creator
#   $3: Token subject
#   $4: Token duration (integer)
#   $5: Token duration unit (default is &quot;day&quot;)
create_jwt_token() {
  local secret=&quot;${1}&quot;
  local jwt_creator_repo=&quot;${2}&quot;
  local subject=${3}
  local duration_int=${4}
  local duration_unit=${5:-day}

  # Clone the Git repository
  git clone &quot;${jwt_creator_repo}&quot; jwt_creator || { echo &quot;Failed to clone repository&quot;; exit 1; }

  # Change to the cloned directory
  cd jwt_creator
  mvn clean install || { echo &quot;Maven build failed&quot;; exit 1; }

  cd target || { echo &quot;Target directory not found&quot;; exit 1; }
  echo &quot;$secret&quot; &gt; secrets.txt

  # Assuming generateJwt.jar is built and exists
  export CREATE_JWT_TOKEN_OUTPUT=$(java -jar generateJwt.jar &quot;secrets.txt&quot; sub &quot;$subject&quot; &quot;$duration_int&quot; &quot;$duration_unit&quot; | grep -v &quot;Generating&quot;)

  # Cleanup
  cd &quot;${WORKSPACE}&quot;
}

# Function to update the WildFly token in standalone.xml
# -----------------------------------------------
# Functionality:
#   1. Updates the token introspection value in WildFly&apos;s standalone.xml using AWS SSM.
#   2. Uses sed to find and replace the token value in the specified file.
# Example Use:
#   replace_wildfly_token &quot;$wildfly_instance_id&quot; &quot;$introspection_token&quot;
# Result:
#   The introspection token in WildFly&apos;s standalone.xml is updated.
replace_wildfly_token() {
    local wildfly_instance_id=$1
    local introspection_token=$2
    update_wildly_standalone_xml_token=&quot;sudo sed -i &apos;s#&lt;simple name=\&quot;java:global/token_introspection_token\&quot; value=\&quot;[^\&quot;]*\&quot;/&gt;#&lt;simple name=\&quot;java:global/token_introspection_token\&quot; value=\&quot;${introspection_token}\&quot;/&gt;#&apos; /home/centos/standalone.xml&quot;
    replace_token_command_json=$(jq -n --arg cmd &quot;$update_wildly_standalone_xml_token&quot; &apos;{commands: [$cmd]}&apos;)
    
    echo &quot;${update_wildly_standalone_xml_token}&quot;
    echo &quot;${replace_token_command_json}&quot;

    # Execute command to update the token using AWS SSM
    command_id=$(aws ssm send-command \
    --instance-ids &quot;${wildfly_instance_id}&quot; \
    --document-name &quot;AWS-RunShellScript&quot; \
    --comment &quot;Update WildFly token&quot; \
    --parameters &quot;$replace_token_command_json&quot; \
    --query &quot;Command.CommandId&quot; \
    --output text)

    wait_for_command ${command_id} ${wildfly_instance_id}
    echo &quot;Token update success&quot;
}

EOF</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>cat &lt;&lt;&apos;EOF&apos; &gt; secret_functions.sh
### Bash functions that need to be migrated to the Bash_Functions job
# Function to fetch a secret from AWS Secrets Manager
fetch_secret() {
    local secret_id=$1
    aws secretsmanager get-secret-value --secret-id &quot;$secret_id&quot;
}

# Function to extract a specific field from a secret
extract_field() {
    local secret=$1
    local field=$2
    echo &quot;$secret&quot; | jq -r .SecretString | jq -r .$field
}
EOF</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>cat &lt;&lt;&apos;EOF&apos; &gt; database_migration_functions.sh
create_flyway_conf() {
  # Parameters:
  # $1 = Database Name
  # $2 = RDS_ENDPOINT
  # $3 = DATABASE_USER
  # $4 = DATABASE_PASSWORD
  # $5 = Placeholders as a string &quot;key1=value1,key2=value2&quot;

  # Define the file path dynamically based on the database name
  CONF_FILE=&quot;conf/$1_flyway.conf&quot;

  # Ensure the conf directory exists
  mkdir -p conf

  # Create or overwrite the flyway.conf file with the new content
  echo &quot;flyway.url=jdbc:mysql://$2/$1&quot; &gt; $CONF_FILE
  echo &quot;flyway.user=$3&quot; &gt;&gt; $CONF_FILE
  echo &quot;flyway.password=$4&quot; &gt;&gt; $CONF_FILE
  echo &quot;flyway.locations=filesystem:/flyway/sql&quot; &gt;&gt; $CONF_FILE
  echo &quot;flyway.baselineVersion=0&quot; &gt;&gt; $CONF_FILE  # Setting the baseline version to 0
  echo &quot;flyway.baselineOnMigrate=true&quot; &gt;&gt; $CONF_FILE  # Automatically baseline at the first migration (V0)

  # Process placeholders if provided
  if [[ -n &quot;$5&quot; ]]; then
    # Split the placeholder string into an array
    IFS=&apos;,&apos; read -r -a placeholders &lt;&lt;&lt; &quot;$5&quot;
    for placeholder in &quot;${placeholders[@]}&quot;; do
      # Each placeholder is in the form key=value
      echo &quot;flyway.placeholders.${placeholder}&quot; &gt;&gt; $CONF_FILE
    done
  fi

  echo &quot;Created or updated $CONF_FILE successfully.&quot;
}


run_flyway_migration() {
  local sql_path=$1
  local flyway_conf_path=$2

  docker run --rm --network=host \
    -v &quot;$(pwd)/${sql_path}:/flyway/sql&quot; \
    -v &quot;$(pwd)/${flyway_conf_path}:/flyway/conf/flyway.conf&quot; \
    flyway/flyway:10.8 -configFiles=/flyway/conf/flyway.conf -X migrate

  # Capture the exit status of the docker command
  local status=$?

  # Check if the status is not zero (which indicates an error)
  if [ $status -ne 0 ]; then
    echo &quot;Error: Flyway migration failed with status $status&quot;
    exit $status  # Exit the script with the same status, causing the Jenkins job to fail
  fi
}
EOF</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>cat &lt;&lt;&apos;EOF&apos; &gt; database_functions.sh
# Global variables for shared database configuration
RDS_ENDPOINT=&quot;&quot;
DATABASE_USER=&quot;&quot;
DATABASE_PASSWORD=&quot;&quot;
MYSQL_IMAGE=&quot;mysql:8.0.35&quot;

# Function to initialize shared database configuration
initialize_shared_db_config() {
    RDS_ENDPOINT=$1
    DATABASE_USER=$2
    DATABASE_PASSWORD=$3
}

# Function to unset shared database configuration
unset_shared_db_config() {
    RDS_ENDPOINT=&quot;&quot;
    DATABASE_USER=&quot;&quot;
    DATABASE_PASSWORD=&quot;&quot;
}

# grant_user_permissions &quot;database_name&quot; &quot;user_name&quot; &quot;user_host&quot;
grant_user_permissions() {
  local database_name=&quot;$1&quot;
  local user_name=&quot;$2&quot;

  # SQL command to grant permissions
  local grant_sql=&quot;GRANT SELECT, INSERT, UPDATE, DELETE ON ${database_name}.* TO &apos;${user_name}&apos;@&apos;%&apos;;&quot;

  # Execute the SQL command using Docker and MySQL client
  docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
    -e &quot;$grant_sql&quot; &quot;mysql&quot;

  # Apply the changes
  docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
    -e &quot;FLUSH PRIVILEGES;&quot; &quot;mysql&quot;
}

# Flyway cannot create databases. Check if a database exists and create it if not
create_database_if_not_exists() {
  # Parameters: $1 = DATABASE_NAME

  # Use printf to properly quote the password and the entire command
  CMD=$(printf &quot;mysql -h%s -u%s -p&apos;%s&apos; -e &apos;CREATE DATABASE IF NOT EXISTS %s;&apos;&quot; &quot;$RDS_ENDPOINT&quot; &quot;$DATABASE_USER&quot; &quot;$DATABASE_PASSWORD&quot; &quot;$1&quot;)

  # Run the command in a Docker container
  docker run --rm $MYSQL_IMAGE /bin/bash -c &quot;$CMD&quot;
}

# Function to drop all tables in a given database
# -----------------------------------------------
# Functionality: 
#   Drops all tables in the specified schema, but exits gracefully if the schema doesn&apos;t exist.
# Example Use:
#   drop_all_tables &quot;auth2&quot;
# Result:
#   All tables in the `auth2` schema are dropped if the schema exists; otherwise, the function exits gracefully.
drop_all_tables() {
    local schema_name=$1

    # Check if the schema exists
    SCHEMA_EXISTS=$(docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
        -sse &quot;SELECT SCHEMA_NAME FROM information_schema.schemata WHERE SCHEMA_NAME = &apos;$schema_name&apos;;&quot; 2&gt;/dev/null)

    if [ -z &quot;$SCHEMA_EXISTS&quot; ]; then
        echo &quot;Schema $schema_name does not exist. Exiting.&quot;
        return 0
    fi

    # If schema exists, proceed to drop all tables
    echo &quot;Dropping all tables in schema: $schema_name...&quot;

    # Fetch list of tables in the schema
    TABLES=$(docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
        --database=$schema_name -Nse &quot;SHOW TABLES;&quot; 2&gt;/dev/null)

    # Drop each table
    for TABLE in $TABLES; do
        echo &quot;Dropping Table: $TABLE&quot;
        DROP_CMD=&quot;SET FOREIGN_KEY_CHECKS = 0; DROP TABLE IF EXISTS $TABLE; SET FOREIGN_KEY_CHECKS = 1;&quot;
        docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
            --database=$schema_name -e &quot;$DROP_CMD&quot; 2&gt;/dev/null
    done

    echo &quot;All tables in $schema_name dropped.&quot;
}

# Function to drop a schema in the database
# -----------------------------------------------
# Functionality: 
#   Drops the specified schema if it exists, but exits gracefully if the schema doesn&apos;t exist.
# Example Use:
#   drop_schema &quot;auth2&quot;
# Result:
#   The `auth2` schema is dropped if it exists; otherwise, the function exits gracefully.
drop_schema() {
    local schema_name=$1

    # Check if the schema exists
    SCHEMA_EXISTS=$(docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
        -sse &quot;SELECT SCHEMA_NAME FROM information_schema.schemata WHERE SCHEMA_NAME = &apos;$schema_name&apos;;&quot; 2&gt;/dev/null)

    if [ -z &quot;$SCHEMA_EXISTS&quot; ]; then
        echo &quot;Schema $schema_name does not exist. Exiting.&quot;
        return 0
    fi

    # If schema exists, proceed to drop the schema
    echo &quot;Dropping schema: $schema_name...&quot;

    DROP_SCHEMA_CMD=&quot;DROP SCHEMA IF EXISTS $schema_name;&quot;
    docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
        -e &quot;$DROP_SCHEMA_CMD&quot; 2&gt;/dev/null

    echo &quot;Schema $schema_name dropped.&quot;
}



# create_top_admin &quot;email&quot; &quot;connection_id&quot;
create_top_admin() {
  docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
  -e &quot;CALL CreateSuperUser(&apos;$1&apos;, &apos;$2&apos;);&quot; &quot;auth&quot;
}

does_user_exists() {
	# $1 email
    # $2 connection_id

    local query_result=$(docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
    -s -r -e &quot;select email from user where email = &apos;$1&apos; and connectionId = (select uuid from connection where id = &apos;$2&apos;);&quot; &quot;auth&quot;)

    echo &quot;$query_result&quot;
}

# update_token_by_uuid &quot;uuid&quot; &quot;new_token&quot;
update_token_by_uuid() {
  local uuid=&quot;$1&quot;
  local new_token=&quot;$2&quot;
  local target_database=&quot;${3:-auth}&quot;

  # Perform the update operation
  docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
    -e &quot;UPDATE application SET token=&apos;$new_token&apos; WHERE uuid=UNHEX(&apos;$uuid&apos;);&quot; &quot;auth&quot;
}

# get_token_by_uuid &quot;uuid&quot;
get_token_by_uuid() {
  local uuid=&quot;$1&quot;
  local schema=&quot;${2:-auth}&quot;

  # Query to get the token by UUID
  local query=&quot;SELECT token FROM application WHERE uuid=UNHEX(REPLACE(&apos;$uuid&apos;, &apos;-&apos;, &apos;&apos;));&quot;

  # Execute the query using Docker and MySQL client
  local token=$(docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
    -sse &quot;$query&quot; &quot;$schema&quot;)

  echo &quot;$token&quot;
}

# run_sql_script &quot;/path/to/your/sql_script.sql&quot; &quot;database_name&quot;
run_sql_script() {
  # Parameters:
  # $1 = PATH_TO_SQL_SCRIPT
  # $2 = DATABASE_NAME

  # Ensure the SQL script file exists
  if [ ! -f &quot;$1&quot; ]; then
    echo &quot;SQL script file does not exist: $1&quot;
    return 1
  fi

  # Run the SQL script in a Docker container
  docker run --rm -v &quot;$1:/sql_script.sql&quot; $MYSQL_IMAGE \
    mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; &quot;$2&quot; &lt; /sql_script.sql
}

# Function to back up a schema with a timestamp
# -----------------------------------------------
# Functionality: 
#   Creates a MySQL dump of the specified schema and saves it with a timestamp in the file name.
# Example Use:
#   backup_schema &quot;auth&quot;
# Result:
#   The live schema `auth` is backed up into a file named something like `auth_20240912123045.sql`.
backup_schema() {
    local schema_to_backup=$1
    local backup_file=&quot;$(date +&quot;%Y%m%d%H%M%S&quot;)_${schema_to_backup}_backup.sql&quot;

    docker run --rm -e MYSQL_PWD=$DATABASE_PASSWORD $MYSQL_IMAGE \
        mysqldump --set-gtid-purged=OFF --single-transaction -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; $schema_to_backup &gt; &quot;$backup_file&quot;

    if [ $? -eq 0 ]; then
        echo &quot;$backup_file&quot;
    else
        echo &quot;Error&quot;
    fi
}

# Function to restore a schema from a dump file stored in S3
# -----------------------------------------------
# Functionality: 
#   1. Downloads the specified SQL dump file from an S3 bucket.
#   2. Restores the schema from the downloaded SQL dump file.
#   The schema will be dropped and recreated before restoration.
# Example Use:
#   restore_schema &quot;auth2&quot; &quot;auth2_dump.sql&quot;
# Result:
#   The `auth2` schema is restored from the specified SQL dump file stored in S3.
# Important Information:
#	This function cannot be executed with assume_role
restore_schema() {
    local schema_name=$1
    local backup_filename=$2
    local s3_bucket=$stack_s3_bucket # The S3 bucket where the dump file is stored

    # Temporary local path to store the dump file
    local temp_dump_file=&quot;/tmp/$backup_filename&quot;

    # Check if the S3 bucket and backup file are provided
    if [ -z &quot;$s3_bucket&quot; ] || [ -z &quot;$backup_filename&quot; ]; then
        echo &quot;S3 bucket or backup file not provided. Exiting.&quot;
        return 1
    fi

    echo &quot;Downloading the dump file $backup_filename from S3 bucket $s3_bucket...&quot;

    # Download the dump file from S3
    aws s3 cp &quot;s3://$s3_bucket/database_backups/$backup_filename&quot; &quot;$temp_dump_file&quot; --sse AES256
    if [ $? -ne 0 ]; then
        echo &quot;Failed to download the dump file from S3. Exiting.&quot;
        return 1
    fi

    # Check if the dump file was downloaded successfully
    if [ ! -f &quot;$temp_dump_file&quot; ]; then
        echo &quot;Dump file $temp_dump_file does not exist after download. Exiting.&quot;
        return 1
    fi

    echo &quot;Restoring schema: $schema_name from dump file: $temp_dump_file...&quot;

    # Drop the schema if it exists and recreate it
    DROP_SCHEMA_CMD=&quot;DROP SCHEMA IF EXISTS $schema_name; CREATE SCHEMA $schema_name;&quot;
    docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
        -e &quot;$DROP_SCHEMA_CMD&quot; 2&gt;/dev/null

    # Restore the schema from the downloaded dump file
    docker run --rm -i $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; &quot;$schema_name&quot; &lt; &quot;$temp_dump_file&quot; 2&gt;/dev/null

    echo &quot;Schema $schema_name restored successfully from $backup_filename.&quot;

    # Cleanup: Remove the downloaded dump file
    rm -f &quot;$temp_dump_file&quot;
}


# Function to copy all objects from one database to another
# -----------------------------------------------
# Functionality: 
#   1. Creates the target database if it doesn&apos;t exist.
#   2. Backs up the target database (if it already existed).
#   3. Drops all tables in the target database using the existing drop_all_tables function.
#   4. Copies all objects (tables and data) from the source database to the target database.
# Example Use:
#   copy_database &quot;auth&quot; &quot;auth2&quot;
# Result:
#   The `auth2` database is created if it doesn&apos;t exist, backed up (if it existed), its tables are dropped, and all objects from the `auth` database are copied into it.
copy_database() {
    local source_database=$1
    local target_database=$2
    
    echo &quot;Starting database copy from $source_database to $target_database...&quot;

    # Step 1: Check if the target database exists
    DATABASE_EXISTS=$(docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
        -sse &quot;SELECT SCHEMA_NAME FROM information_schema.schemata WHERE SCHEMA_NAME = &apos;$target_database&apos;;&quot;)

    if [ -z &quot;$DATABASE_EXISTS&quot; ]; then
        # If the target database doesn&apos;t exist, create it
        echo &quot;Target database $target_database does not exist. Creating database...&quot;
        docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
            -e &quot;CREATE DATABASE IF NOT EXISTS $target_database;&quot;

        if [ $? -eq 0 ]; then
            echo &quot;Database $target_database created successfully.&quot;
        else
            echo &quot;Error occurred while creating $target_database.&quot;
            exit 1
        fi
    else
        # If the target database exists, back it up
        echo &quot;Backing up $target_database database...&quot;
        backup_schema &quot;$target_database&quot;
    fi

    # Step 2: Drop all tables in the target database using the existing drop_all_tables function
    echo &quot;Dropping all tables in $target_database...&quot;
    drop_all_tables &quot;$target_database&quot;
    echo &quot;All tables in $target_database dropped.&quot;

    # Step 3: Copy all tables and data from the source database to the target database
    echo &quot;Copying all tables from $source_database to $target_database...&quot;

    # Get the list of tables from the source database and execute SQL commands to copy structure and data
    TABLES=$(docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
        --skip-column-names -e &quot;SELECT table_name FROM information_schema.tables WHERE table_schema = &apos;$source_database&apos;;&quot;)

    for TABLE in $TABLES; do
        # Create table in the target database
        echo &quot;Creating table $TABLE in $target_database...&quot;
        docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
            -e &quot;CREATE TABLE $target_database.$TABLE LIKE $source_database.$TABLE;&quot;

        # Copy data from the source table to the target table
        echo &quot;Copying data from $source_database.$TABLE to $target_database.$TABLE...&quot;
        docker run --rm $MYSQL_IMAGE mysql -h&quot;$RDS_ENDPOINT&quot; -u&quot;$DATABASE_USER&quot; -p&quot;$DATABASE_PASSWORD&quot; \
            -e &quot;INSERT INTO $target_database.$TABLE SELECT * FROM $source_database.$TABLE;&quot;
    done

    echo &quot;Database copy from $source_database to $target_database completed successfully.&quot;
}
EOF</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>cat &lt;&lt;&apos;EOF&apos; &gt; string_util_functions.sh
# Function to convert a comma-separated list of domains into a regex pattern
# Will produce ^https?://(www\.)?(www\.domain1\.com|www\.domain2\.com|www\.domain3\.org)
generate_domain_regex() {
  local IFS=&apos;,&apos; # Set the internal field separator to comma
  read -ra ADDR &lt;&lt;&lt; &quot;$1&quot; # Read the list into an array
  local regex=&quot;^https?://(www\.)?(&quot;

  # Loop through the array and append each domain to the regex pattern
  for domain in &quot;${ADDR[@]}&quot;; do
    regex+=&quot;${domain//./\\.}|&quot;
  done

  # Remove the last &apos;|&apos; and close the pattern, allowing for any paths
  regex=&quot;${regex%|}).*&quot;
  echo &quot;${regex}&quot;
}

replace_xml_special_chars() {
    local input_string=&quot;$1&quot;
    local output_string=&quot;&quot;

    # Length of the input string
    local len=${#input_string}
    local i char ord

    for ((i=0; i&lt;len; i++)); do
        char=&quot;${input_string:i:1}&quot;
        case &quot;$char&quot; in
            &apos;&amp;&apos;)
                output_string+=&quot;&amp;amp;&quot;
                ;;
            &apos;&lt;&apos;)
                output_string+=&quot;&amp;lt;&quot;
                ;;
            &apos;&gt;&apos;)
                output_string+=&quot;&amp;gt;&quot;
                ;;
            &apos;&quot;&apos;)
                output_string+=&quot;&amp;quot;&quot;
                ;;
            &quot;&apos;&quot;)
                output_string+=&quot;&amp;apos;&quot;
                ;;
            &apos;?&apos;)
                output_string+=&quot;&amp;#63;&quot;
                ;;
            &apos;#&apos;)
                output_string+=&quot;&amp;#35;&quot;
                ;;
            &apos;{&apos;)
                output_string+=&quot;&amp;#123;&quot;
                ;;
            *)
                output_string+=&quot;$char&quot;
                ;;
        esac
    done

    echo &quot;$output_string&quot;
}
EOF</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>cat &lt;&lt;&apos;EOF&apos; &gt; helper_functions.sh

# Function to wait for command to complete and check for success
wait_for_command() {
  local command_id=$1
  local instance_id=$2

  while true; do
    command_status=$(aws ssm get-command-invocation --command-id ${command_id} --instance-id ${instance_id} --query &apos;Status&apos; --output text)

    if [[ &quot;${command_status}&quot; == &quot;Success&quot; ]]; then
      echo &quot;Command succeeded&quot;
      break
    elif [[ &quot;${command_status}&quot; == &quot;Failed&quot; || &quot;${command_status}&quot; == &quot;Cancelled&quot; || &quot;${command_status}&quot; == &quot;TimedOut&quot; ]]; then
      echo &quot;Command failed with status: ${command_status}&quot;
      echo &quot;Fetching logs...&quot;

      aws ssm get-command-invocation --command-id ${command_id} --instance-id ${instance_id} --query &apos;StandardOutputContent&apos; --output text &gt; command_output.log
      aws ssm get-command-invocation --command-id ${command_id} --instance-id ${instance_id} --query &apos;StandardErrorContent&apos; --output text &gt; command_error.log

      echo &quot;Standard Output:&quot;
      cat command_output.log
      echo &quot;Standard Error:&quot;
      cat command_error.log

      exit 1
    else
      echo &quot;Command status: ${command_status}, waiting...&quot;
      sleep 5
    fi
  done
}
EOF</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>cat &lt;&lt;&apos;EOF&apos; &gt; application_functions.sh
# Function to restart the WildFly container
# -----------------------------------------------
# Functionality:
#   1. Restarts the WildFly container via AWS SSM by executing a Docker restart command.
# Example Use:
#   restart_wildfly &quot;$wildfly_instance_id&quot;
# Result:
#   The WildFly container is restarted successfully.
restart_wildfly() {
    local wildfly_instance_id=$1
    restart_command=&quot;sudo docker restart wildfly&quot;
    restart_command_json=$(jq -n --arg cmd &quot;$restart_command&quot; &apos;{commands: [$cmd]}&apos;)

    command_id=$(aws ssm send-command \
    --instance-ids &quot;${wildfly_instance_id}&quot; \
    --document-name &quot;AWS-RunShellScript&quot; \
    --comment &quot;Restart WildFly container&quot; \
    --parameters &quot;$restart_command_json&quot; \
    --query &quot;Command.CommandId&quot; \
    --output text)

    wait_for_command ${command_id} ${wildfly_instance_id}
}

# Function to update the WildFly token in standalone.xml
# -----------------------------------------------
# Functionality:
#   1. Compares the existing token in WildFly&apos;s standalone.xml with the new introspection token.
#   2. If the tokens are the same, no update is made.
#   3. If the tokens are different, the new token is updated in the standalone.xml file using AWS SSM and the
#	WildFly docker container is restarted.
# Example Use:
#   update_wildfly_token &quot;$wildfly_instance_id&quot; &quot;$introspection_token&quot;
# Result:
#   The introspection token in WildFly&apos;s standalone.xml is updated only if it&apos;s different from the existing token.
update_wildfly_token() {
    local wildfly_instance_id=$1
    local introspection_token=$2

    # Command to extract the current token from standalone.xml
    get_current_token_command=&quot;grep &apos;&lt;simple name=\&quot;java:global/token_introspection_token\&quot;&apos; /home/centos/standalone.xml | sed -n &apos;s/.*value=\&quot;\\([^\&quot;]*\\)\&quot;.*/\\1/p&apos;&quot;
    
    # Format the command in JSON
    get_token_command_json=$(jq -n --arg cmd &quot;$get_current_token_command&quot; &apos;{commands: [$cmd]}&apos;)
    
    # Execute the command using AWS SSM and wait for immediate response
    command_id=$(aws ssm send-command \
        --instance-ids &quot;${wildfly_instance_id}&quot; \
        --document-name &quot;AWS-RunShellScript&quot; \
        --parameters &quot;$get_token_command_json&quot; \
        --query &quot;Command.CommandId&quot; \
        --output text)
    
    # Immediately fetch the command output after execution
    current_token=$(aws ssm get-command-invocation \
        --command-id ${command_id} \
        --instance-id ${wildfly_instance_id} \
        --query &apos;StandardOutputContent&apos; \
        --output text)

    echo &quot;Current token: $current_token&quot;
    echo &quot;New token: $introspection_token&quot;

    # Compare the current token with the new token
    if [ &quot;$current_token&quot; = &quot;$introspection_token&quot; ]; then
        echo &quot;Tokens are identical. No update required.&quot;
    else
        echo &quot;Tokens differ. Updating token.&quot;

        # Update WildFly standalone.xml token
        update_wildly_standalone_xml_token=&quot;sudo sed -i &apos;s#&lt;simple name=\&quot;java:global/token_introspection_token\&quot; value=\&quot;[^\&quot;]*\&quot;/&gt;#&lt;simple name=\&quot;java:global/token_introspection_token\&quot; value=\&quot;${introspection_token}\&quot;/&gt;#&apos; /home/centos/standalone.xml&quot;
        replace_token_command_json=$(jq -n --arg cmd &quot;$update_wildly_standalone_xml_token&quot; &apos;{commands: [$cmd]}&apos;)

        echo &quot;${update_wildly_standalone_xml_token}&quot;
        echo &quot;${replace_token_command_json}&quot;

        # Execute command to update the token using AWS SSM
        command_id=$(aws ssm send-command \
        --instance-ids &quot;${wildfly_instance_id}&quot; \
        --document-name &quot;AWS-RunShellScript&quot; \
        --comment &quot;Update WildFly token&quot; \
        --parameters &quot;$replace_token_command_json&quot; \
        --query &quot;Command.CommandId&quot; \
        --output text)

        wait_for_command ${command_id} ${wildfly_instance_id}
        echo &quot;Token update success&quot;
    fi
    
    restart_wildfly &quot;$wildfly_instance_id&quot;
}

# Function to monitor Docker logs via SSM to detect if Spring Boot has started
# -----------------------------------------------
# Functionality:
#   1. Uses AWS SSM to execute &apos;docker logs&apos; periodically on the remote EC2 instance.
#   2. Monitors the logs for the &quot;Started Application in&quot; message every 5 seconds.
# Example Use:
#   wait_for_spring_boot_ssm_logs &quot;i-xxxxxxxxxx&quot; &quot;spring_app_container&quot;
# Result:
#   The function will wait until the log message indicating the application has started is found.
wait_for_spring_boot_ssm_logs() {
  local instance_id=$1    # EC2 instance ID where the Docker container is running
  local container_name=$2 # Docker container name or ID
  local phrase=&quot;Started Application in&quot;
  local max_retries=${3:-60}  # Maximum number of retries before giving up (default 60 retries)
  local sleep_duration=${4:-5} # Wait 5 seconds between each retry

  echo &quot;Waiting for Spring Boot application to start by monitoring Docker logs via SSM...&quot;

  for ((i=1; i&lt;=max_retries; i++)); do
    # Command to check the Docker logs for the startup phrase
    check_logs_command=&quot;docker logs ${container_name} 2&gt;&amp;1 | grep &apos;${phrase}&apos;&quot;

    # Execute the command using AWS SSM
    check_logs_command_json=$(jq -n --arg cmd &quot;$check_logs_command&quot; &apos;{commands: [$cmd]}&apos;)

    command_id=$(aws ssm send-command \
      --instance-ids &quot;${instance_id}&quot; \
      --document-name &quot;AWS-RunShellScript&quot; \
      --parameters &quot;$check_logs_command_json&quot; \
      --comment &quot;Check Docker logs for Spring Boot startup&quot; \
      --query &quot;Command.CommandId&quot; \
      --output text)

    # Fetch the result of the command
    log_output=$(aws ssm get-command-invocation \
      --command-id ${command_id} \
      --instance-id ${instance_id} \
      --query &apos;StandardOutputContent&apos; \
      --output text)

    # Check if the phrase was found in the logs
    if echo &quot;$log_output&quot; | grep -q &quot;${phrase}&quot;; then
      echo &quot;Spring Boot application started in the Docker container &apos;${container_name}&apos;!&quot;
      break
    elif [[ &quot;$i&quot; -eq &quot;$max_retries&quot; ]]; then
      echo &quot;Spring Boot application failed to start within the maximum retries (${max_retries}).&quot;
      exit 1
    else
      echo &quot;Spring Boot application not started yet. Retrying in ${sleep_duration} seconds... ($i/$max_retries)&quot;
      sleep ${sleep_duration}
    fi
  done
}



EOF</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>cat &lt;&lt;&apos;EOF&apos; &gt; terraform_functions.sh
# Function to download the Terraform state file for a given stack
# -----------------------------------------------
# Functionality:
#   1. Downloads the Terraform state file from the specified S3 bucket for the provided stack.
#   2. Saves the file locally with the naming convention terraform_&lt;stack&gt;.tfstate.
# Example Use:
#   tf_download_state &quot;a&quot; &quot;my-stack-s3-bucket&quot;
# Result:
#   The Terraform state file for stack &quot;a&quot; is downloaded from the S3 bucket and saved as terraform_a.tfstate.
tf_download_state() {
    local stack=$1
    local stack_s3_bucket=$2
    echo &quot;Download terraform.tfstate file for ${stack} stack&quot;
    aws s3 cp s3://$stack_s3_bucket/deployment_state_metadata/${stack}/terraform.tfstate terraform_${stack}.tfstate
}

# Function to extract the WildFly EC2 instance ID from the Terraform state
# -----------------------------------------------
# Functionality:
#   1. Extracts the WildFly EC2 instance ID from the downloaded Terraform state file.
#   2. The instance ID is extracted using jq and returned for further use.
# Example Use:
#   wildfly_instance_id=$(tf_extract_wildfly_instance_id &quot;a&quot;)
# Result:
#   The WildFly EC2 instance ID for stack &quot;a&quot; is returned.
tf_extract_wildfly_instance_id() {
    local stack=$1
    local wildfly_instance_id=$(jq -r &apos;.outputs[&quot;wildfly-ec2-id&quot;].value&apos; terraform_${stack}.tfstate)
    echo &quot;${wildfly_instance_id}&quot;
}

EOF</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
  </builders>
  <publishers>
    <hudson.tasks.ArtifactArchiver>
      <artifacts>target_group_attachment_functions.sh, role_functions.sh, pic_sure_rds_snapshot.sh, jwt_token.sh, database_migration_functions.sh, secret_functions.sh, database_functions.sh, helper_functions.sh, application_functions.sh, terraform_functions.sh</artifacts>
      <allowEmptyArchive>false</allowEmptyArchive>
      <onlyIfSuccessful>false</onlyIfSuccessful>
      <fingerprint>false</fingerprint>
      <defaultExcludes>true</defaultExcludes>
      <caseSensitive>true</caseSensitive>
      <followSymlinks>false</followSymlinks>
    </hudson.tasks.ArtifactArchiver>
  </publishers>
  <buildWrappers>
    <hudson.plugins.ws__cleanup.PreBuildCleanup plugin="ws-cleanup@0.46">
      <deleteDirs>false</deleteDirs>
      <cleanupParameter></cleanupParameter>
      <externalDelete></externalDelete>
      <disableDeferredWipeout>false</disableDeferredWipeout>
    </hudson.plugins.ws__cleanup.PreBuildCleanup>
  </buildWrappers>
</project>