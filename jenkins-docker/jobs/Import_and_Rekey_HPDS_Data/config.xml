<?xml version='1.1' encoding='UTF-8'?>
<project>
  <actions/>
  <description></description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>source_s3_url</name>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>output_file_name</name>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>destination_bucket_hash</name>
          <description>the git hash of the commit that changed the data set</description>
          <trim>true</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.BooleanParameterDefinition>
          <name>isDestigmatized</name>
          <defaultValue>false</defaultValue>
        </hudson.model.BooleanParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
  </properties>
  <scm class="hudson.plugins.git.GitSCM" plugin="git@5.7.0">
    <configVersion>2</configVersion>
    <userRemoteConfigs>
      <hudson.plugins.git.UserRemoteConfig>
        <url>${release_control_git_repo}</url>
      </hudson.plugins.git.UserRemoteConfig>
    </userRemoteConfigs>
    <branches>
      <hudson.plugins.git.BranchSpec>
        <name>${release_control_git_hash}</name>
      </hudson.plugins.git.BranchSpec>
    </branches>
    <doGenerateSubmoduleConfigurations>false</doGenerateSubmoduleConfigurations>
    <submoduleCfg class="empty-list"/>
    <extensions/>
  </scm>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <jdk>JDK-21</jdk>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash
set -e

# Source folder containing the scripts
source_scripts_folder=&quot;${JENKINS_HOME}/workspace/Bash_Functions/&quot;
ls -la &quot;$source_scripts_folder&quot;

# Iterate through the files in the folder and source them
for script_file in &quot;$source_scripts_folder&quot;*.sh; do
    chmod +x &quot;$script_file&quot;
    if [ -f &quot;$script_file&quot; ] &amp;&amp; [ -x &quot;$script_file&quot; ]; then
        echo &quot;sourcing $script_file&quot;
        source &quot;$script_file&quot;
    fi
done

stack=$(get_stack &quot;staging&quot;)

### Build ETL Image
aws s3 --sse=AES256 cp s3://${stack_s3_bucket}/${stack}/containers/pic-sure-hpds-etl.tar.gz pic-sure-hpds-etl.tar.gz

HPDS_ETL_IMAGE=$(docker load &lt; ./pic-sure-hpds-etl.tar.gz | cut -d &apos; &apos; -f 3)
echo $HPDS_ETL_IMAGE
ls -alh &quot;$(pwd)&quot;

mkdir -p local/hpds/
mkdir -p local/source/

### Pull data set to rekey.
assume_role &quot;arn:aws:iam::736265540791:role/curated-datasets-s3-role&quot;
aws s3 cp ${source_s3_url} local/source/javabins.tar

### Try extracting as compressed tar
if tar -xzvf local/source/javabins.tar -C local/source; then
    echo &quot;Extracted compressed tar file.&quot;
else
    echo &quot;Compressed extraction failed, trying non-compressed...&quot;
    if tar -xvf local/source/javabins.tar -C local/source; then
        echo &quot;Extracted non-compressed tar file.&quot;
    else
        echo &quot;Error: Extraction failed for both compressed and non-compressed tar files.&quot;
        exit 1
    fi
fi

reset_role
### Move meta into local/hpds to include with new data set
cp local/source/metadata.json local/hpds/

### Generate rekey encryption key to container&apos;s as local/hpds/encryption_key
head /dev/urandom | tr -dc a-f0-9 | head -c 32 &gt; &quot;$WORKSPACE&quot;/local/hpds/encryption_key

#### Execute rekey
# volume mount pwd that contains original data files as /opt/local/source/
docker run -i -e HEAPSIZE=20480 -e JAVA_OPTS=&quot;-Xms4g -XX:+UseG1GC&quot; -v &quot;${WORKSPACE}&quot;/local:/opt/local:Z -e LOADER_NAME=RekeyDataset $HPDS_ETL_IMAGE

### If destigmatized data set, remove concepts
if [ &quot;${isDestigmatized}&quot; = &apos;true&apos; ]; then
    # RemoveConceptFromMetadata reads the rekeyed data set in /opt/local/hpds generated by the rekey.
    # add concepts to remove to /opt/local/hpds that contains the list of concepts to destigmatize
    cp local/source/conceptsToRemove.txt local/hpds/
    docker run -i -v &quot;${WORKSPACE}&quot;/local:/opt/local:Z -e LOADER_NAME=RemoveConceptFromMetadata $HPDS_ETL_IMAGE
fi

cd &quot;$WORKSPACE&quot;/local/hpds/

# Leave the data set artifact as non-compressed for the rest of it&apos;s lifecycle
tar -cvf &quot;${output_file_name}.tar&quot; *.javabin encryption_key metadata.json

aws s3 --sse=AES256 cp &quot;${output_file_name}.tar&quot; s3://${stack_s3_bucket}/data/${destination_bucket_hash}/${output_file_name}.tar --quiet
aws s3 --sse=AES256 cp metadata.json s3://${stack_s3_bucket}/data/${destination_bucket_hash}/fence_mapping.json
</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers>
    <hudson.plugins.ws__cleanup.PreBuildCleanup plugin="ws-cleanup@0.49">
      <deleteDirs>false</deleteDirs>
      <cleanupParameter></cleanupParameter>
      <externalDelete></externalDelete>
      <disableDeferredWipeout>false</disableDeferredWipeout>
    </hudson.plugins.ws__cleanup.PreBuildCleanup>
  </buildWrappers>
</project>