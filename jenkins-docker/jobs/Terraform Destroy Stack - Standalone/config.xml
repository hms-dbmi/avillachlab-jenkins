<?xml version='1.1' encoding='UTF-8'?>
<project>
  <actions/>
  <description>This job will destroy a terraform state for the specified stack passed in. &#xd;
&#xd;
This job presumes the stack is running and is registered with a target group.</description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>stack_1</name>
          <defaultValue>a</defaultValue>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>stack_2</name>
          <defaultValue>b</defaultValue>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.BooleanParameterDefinition>
          <name>destroy_green</name>
          <defaultValue>false</defaultValue>
        </hudson.model.BooleanParameterDefinition>
        <hudson.model.BooleanParameterDefinition>
          <name>destroy_blue</name>
          <defaultValue>false</defaultValue>
        </hudson.model.BooleanParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
  </properties>
  <scm class="hudson.plugins.git.GitSCM" plugin="git@5.6.0">
    <configVersion>2</configVersion>
    <userRemoteConfigs>
      <hudson.plugins.git.UserRemoteConfig>
        <url>${infrastructure_git_repo}</url>
      </hudson.plugins.git.UserRemoteConfig>
    </userRemoteConfigs>
    <branches>
      <hudson.plugins.git.BranchSpec>
        <name>*/master</name>
      </hudson.plugins.git.BranchSpec>
    </branches>
    <doGenerateSubmoduleConfigurations>false</doGenerateSubmoduleConfigurations>
    <submoduleCfg class="empty-list"/>
    <extensions/>
  </scm>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <jdk>(System)</jdk>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash

# Source folder containing the scripts
source_scripts_folder=&quot;${JENKINS_HOME}/workspace/Bash_Functions/&quot;
ls -la &quot;$source_scripts_folder&quot;

# Iterate through the files in the folder and source them
for script_file in &quot;$source_scripts_folder&quot;*.sh; do
    chmod +x &quot;$script_file&quot;
    if [ -f &quot;$script_file&quot; ] &amp;&amp; [ -x &quot;$script_file&quot; ]; then
        echo &quot;sourcing $script_file&quot;
        source &quot;$script_file&quot;
    fi
done
cd app-infrastructure

# Destroy the green stack.
aws s3 cp s3://$stack_s3_bucket/deployment_state_metadata/${stack_1}/terraform.tfstate terraform.tfstate
terraform init
private_ips_from_state_1=($(terraform show -json terraform.tfstate | jq -r &apos;.values.root_module.resources[] | select(.type == &quot;aws_instance&quot;).values.private_ip&apos;))
httpd_private_ip_from_state_1=$(terraform show -json terraform.tfstate | jq -r &apos;.values.root_module.resources[] | select(.type == &quot;aws_instance&quot; and .values.tags.Node == &quot;HTTPD&quot;).values.private_ip&apos;)
echo &quot;private ip for $stack_1: $private_ips_from_state_1&quot;

echo &quot;checking Stack: $stack_2&quot;
aws s3 cp s3://$stack_s3_bucket/deployment_state_metadata/${stack_2}/terraform.tfstate terraform.tfstate
terraform init
private_ips_from_state_2=($(terraform show -json terraform.tfstate | jq -r &apos;.values.root_module.resources[] | select(.type == &quot;aws_instance&quot;).values.private_ip&apos;))
httpd_private_ip_from_state_2=$(terraform show -json terraform.tfstate | jq -r &apos;.values.root_module.resources[] | select(.type == &quot;aws_instance&quot; and .values.tags.Node == &quot;HTTPD&quot;).values.private_ip&apos;)
echo &quot;private ip for $stack_2: $private_ips_from_state_2&quot;

# local vars
assume_role
staging_target_group_arn=$(get_target_group_arn_by_name &quot;$staging_tg_name&quot;)
live_target_group_arn=$(get_target_group_arn_by_name &quot;$live_tg_name&quot;)

staging_group_vpc=$(get_target_group_vpc_by_tg_name $staging_tg_name)
live_group_vpc=$(get_target_group_vpc_by_tg_name $staging_tg_name)

staging_private_ips=($(aws elbv2 describe-target-health --target-group-arn $staging_target_group_arn --query &apos;TargetHealthDescriptions[*].Target.Id&apos; --output text))
live_private_ips=($(aws elbv2 describe-target-health --target-group-arn $live_target_group_arn --query &apos;TargetHealthDescriptions[*].Target.Id&apos; --output text))
reset_role

#############
# Staging Deregister and Destroy
if [[ &quot;$destroy_green&quot; == &quot;true&quot; ]]; then
	assume_role
    staging_private_ips=($(aws elbv2 describe-target-health --target-group-arn $staging_target_group_arn --query &apos;TargetHealthDescriptions[*].Target.Id&apos; --output text))
    
    staging_stack=
    staging_http_ip=
    
    for ip in &quot;${staging_private_ips[@]}&quot;; do
        if [[ &quot;$ip&quot; == &quot;$httpd_private_ip_from_state_1&quot; ]]; then
            staging_stack=$stack_1
            staging_http_ip=$httpd_private_ip_from_state_1
            break
        elif [[ &quot;$ip&quot; == &quot;$httpd_private_ip_from_state_2&quot; ]]; then
            staging_stack=$stack_2
            staging_http_ip=$httpd_private_ip_from_state_2
            break
        fi
    done
    
    echo &quot;Current Staging Stack: $staging_stack&quot;
    
    reset_role

   if [[ -z $staging_stack ]]; then
      echo &quot;No stack registered or found in the staging target group - $staging_target_group_arn&quot;
      exit 0
   fi
   echo &quot;Deregistering from Staging Target Group&quot;
   
   assume_role
   deregister_targets &quot;$staging_group_vpc&quot; &quot;$staging_target_group_arn&quot; &quot;$staging_http_ip&quot;
   reset_role
   
   echo &quot;Destroying Staging environment&quot;
   aws s3 cp s3://$stack_s3_bucket/deployment_state_metadata/${staging_stack}/terraform.tfstate terraform.tfstate

   terraform init
   
   assume_role

   terraform destroy -auto-approve \
   -var=&quot;dataset_s3_object_key=${dataset_s3_object_key}&quot;  \
   -var=&quot;destigmatized_dataset_s3_object_key=${destigmatized_dataset_s3_object_key}&quot;  \
   -var=&quot;genomic_dataset_s3_object_key=${genomic_dataset_s3_object_key}&quot; \
   -var=&quot;target_stack=$staging_stack&quot; \
   -var=&quot;picsure_rds_snapshot_id=${picsure_rds_snapshot_id}&quot; \
   -var=&quot;stack_githash=`echo ${deployment_git_hash} |cut -c1-7`&quot; \
   -var=&quot;stack_githash_long=${deployment_git_hash}&quot; \
   -var=&quot;env_public_dns_name=${env_public_dns_name}&quot; \
   -var=&quot;env_public_dns_name_staging=${env_public_dns_name_staging}&quot; \
   -var=&quot;env_private_dns_name=${env_private_dns_name}&quot; \
   -var=&quot;env_hosted_zone_id=${env_hosted_zone_id}&quot; \
   -var=&quot;analytics_id=${analytics_id}&quot; \
   -var=&quot;tag_manager_id=${tag_manager_id}&quot; \
   -var=&quot;env_is_open_access=${env_is_open_access}&quot; \
   -var=&quot;include_auth_hpds=${include_auth_hpds}&quot; \
   -var=&quot;include_open_hpds=${include_open_hpds}&quot; \
   -var=&quot;environment_name=${environment_name}&quot; \
   -var=&quot;env_staging_subdomain=${env_staging_subdomain}&quot; \
   -var=&quot;application_id_for_base_query=${application_id_for_base_query}&quot; \
   -var=&quot;stack_s3_bucket=${stack_s3_bucket}&quot; \
   -var=&quot;idp_provider=${idp_provider}&quot; \
   -var=&quot;env_project=${env_project}&quot; \
   -var=&quot;picsure_token_introspection_token=${picsure_token_introspection_token}&quot; \
   -var=&quot;picsure_client_secret=${picsure_client_secret}&quot; \
   -var=&quot;fence_client_id=${fence_client_id}&quot; \
   -var=&quot;fence_client_secret=${fence_client_secret}&quot; \
   -var=&quot;idp_provider_uri=${idp_provider_uri}&quot; || true
   
   reset_role
   
   aws s3 cp terraform.tfstate s3://$stack_s3_bucket/deployment_state_metadata/${staging_stack}/terraform.tfstate
fi

#############
# Production Deregister and Destroy
if [[ &quot;$destroy_blue&quot; == &quot;true&quot; &amp;&amp; &quot;${environment_name,,}&quot; != &quot;prod&quot; ]]; then
    assume_role
    
    live_private_ips=($(aws elbv2 describe-target-health --target-group-arn $live_target_group_arn --query &apos;TargetHealthDescriptions[*].Target.Id&apos; --output text))
    
    live_stack=
    live_http_ip=
    
    for ip in &quot;${live_private_ips[@]}&quot;; do
        if [[ &quot;$ip&quot; == &quot;$httpd_private_ip_from_state_1&quot; ]]; then
            live_stack=$stack_1
            live_http_ip=$httpd_private_ip_from_state_1
            break
        elif [[ &quot;$ip&quot; == &quot;$httpd_private_ip_from_state_2&quot; ]]; then
            live_stack=$stack_2
            live_http_ip=$httpd_private_ip_from_state_2
            break
        fi
    done
    
    echo &quot;Current Live Stack: $live_stack&quot;

   reset_role
   if [[ -z $live_stack ]]; then
      echo &quot;No stack registered or found in the live target group - $live_target_group_arn&quot;
      exit 0
   fi
   echo &quot;Deregistering from live Target Group&quot;
   
   assume_role
   deregister_targets &quot;$live_group_vpc&quot; &quot;$live_target_group_arn&quot; &quot;$live_http_ip&quot;
   reset_role
   
   echo &quot;Destroying live environment&quot;
   aws s3 cp s3://$stack_s3_bucket/deployment_state_metadata/${live_stack}/terraform.tfstate terraform.tfstate
   
   terraform init
   
   assume_role
   
   terraform destroy -auto-approve \
   -var=&quot;dataset_s3_object_key=${dataset_s3_object_key}&quot;  \
   -var=&quot;destigmatized_dataset_s3_object_key=${destigmatized_dataset_s3_object_key}&quot;  \
   -var=&quot;genomic_dataset_s3_object_key=${genomic_dataset_s3_object_key}&quot; \
   -var=&quot;target_stack=$live_stack&quot; \
   -var=&quot;picsure_rds_snapshot_id=${picsure_rds_snapshot_id}&quot; \
   -var=&quot;stack_githash=`echo ${deployment_git_hash} |cut -c1-7`&quot; \
   -var=&quot;stack_githash_long=${deployment_git_hash}&quot; \
   -var=&quot;env_public_dns_name=${env_public_dns_name}&quot; \
   -var=&quot;env_public_dns_name_staging=${env_public_dns_name_staging}&quot; \
   -var=&quot;env_private_dns_name=${env_private_dns_name}&quot; \
   -var=&quot;env_hosted_zone_id=${env_hosted_zone_id}&quot; \
   -var=&quot;analytics_id=${analytics_id}&quot; \
   -var=&quot;tag_manager_id=${tag_manager_id}&quot; \
   -var=&quot;env_is_open_access=${env_is_open_access}&quot; \
   -var=&quot;include_auth_hpds=${include_auth_hpds}&quot; \
   -var=&quot;include_open_hpds=${include_open_hpds}&quot; \
   -var=&quot;environment_name=${environment_name}&quot; \
   -var=&quot;env_staging_subdomain=${env_staging_subdomain}&quot; \
   -var=&quot;application_id_for_base_query=${application_id_for_base_query}&quot; \
   -var=&quot;stack_s3_bucket=${stack_s3_bucket}&quot; \
   -var=&quot;idp_provider=${idp_provider}&quot; \
   -var=&quot;env_project=${env_project}&quot; \
   -var=&quot;picsure_token_introspection_token=${picsure_token_introspection_token}&quot; \
   -var=&quot;picsure_client_secret=${picsure_client_secret}&quot; \
   -var=&quot;fence_client_id=${fence_client_id}&quot; \
   -var=&quot;fence_client_secret=${fence_client_secret}&quot; \
   -var=&quot;idp_provider_uri=${idp_provider_uri}&quot; || true
   
   reset_role
           
   aws s3 cp terraform.tfstate s3://$stack_s3_bucket/deployment_state_metadata/${live_stack}/terraform.tfstate
fi</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers>
    <hudson.plugins.ws__cleanup.PreBuildCleanup plugin="ws-cleanup@0.48">
      <deleteDirs>false</deleteDirs>
      <cleanupParameter></cleanupParameter>
      <externalDelete></externalDelete>
      <disableDeferredWipeout>false</disableDeferredWipeout>
    </hudson.plugins.ws__cleanup.PreBuildCleanup>
  </buildWrappers>
</project>