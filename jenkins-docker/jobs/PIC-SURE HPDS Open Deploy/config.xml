<?xml version='1.1' encoding='UTF-8'?>
<project>
  <actions/>
  <description>Runs the &quot;deploy-open-hpds.sh&quot; script on the Open HPDS EC2 server.</description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>STACK_S3_BUCKET</name>
          <defaultValue>${stack_s3_bucket}</defaultValue>
          <trim>true</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>TARGET_STACK</name>
          <trim>true</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>DESTIGMATIZED_DATASET_S3_OBJECT_KEY</name>
          <description>Optional. Setting this value will update related files, folders, datasets, etc.</description>
          <trim>true</trim>
        </hudson.model.StringParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
  </properties>
  <scm class="hudson.scm.NullSCM"/>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <jdk>(System)</jdk>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash

# Source folder containing the scripts
source_scripts_folder=&quot;${JENKINS_HOME}/workspace/Bash_Functions/&quot;


# Iterate through the files in the folder and source them
for script_file in &quot;$source_scripts_folder&quot;*.sh; do
    chmod +x &quot;$script_file&quot;
    if [ -f &quot;$script_file&quot; ] &amp;&amp; [ -x &quot;$script_file&quot; ]; then
        echo &quot;sourcing $script_file&quot;
        source &quot;$script_file&quot;
    fi
done

assume_role

# Download the terraform state file
echo &quot;Download terraform.tfstate file for $target_stack stack&quot;
aws s3 cp s3://$stack_s3_bucket/deployment_state_metadata/$target_stack/terraform.tfstate terraform.tfstate

# Extract the Wildfly EC2 instance ID
echo &quot;Extract HPDS instance ID for $target_stack stack&quot;
open-hpds_instance_id=$(jq -r &apos;.outputs[&quot;open-hpds-ec2-id&quot;].value&apos; terraform.tfstate)
echo &quot;${open-hpds_instance_id}&quot;

update_open-hpds_container=&quot;sudo /home/centos/deploy-open-hpds.sh \
        --stack_s3_bucket $STACK_S3_BUCKET \
        --destigmatized_dataset_s3_object_key $DESTIGMATIZED_DATASET_S3_OBJECT_KEY \
        --target_stack $TARGET_STACK&quot;

echo &quot;$update_open-hpds_container&quot;
update_open-hpds_container_json=$(jq -n --arg cmd &quot;$update_open-hpds_container&quot; &apos;{commands: [$cmd]}&apos;)
echo &quot;JSON version of run command: $update_open-hpds_container_json&quot;

command_id=$(aws ssm send-command \
  --instance-ids &quot;${open-hpds_instance_id}&quot; \
  --document-name &quot;AWS-RunShellScript&quot; \
  --comment &quot;Deploy open-hpds in place&quot; \
  --parameters &quot;$update_open-hpds_container_json&quot; \
  --query &quot;Command.CommandId&quot; \
  --output text \
  --debug)

wait_for_command ${command_id} ${open-hpds_instance_id}

reset_role
</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers/>
</project>