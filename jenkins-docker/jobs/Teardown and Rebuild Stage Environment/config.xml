<?xml version='1.1' encoding='UTF-8'?>
<project>
  <actions/>
  <description></description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>live_stack</name>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>target_stack</name>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>deployment_git_hash</name>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>ROLE_ARN</name>
          <defaultValue>arn:aws:iam::${app_acct_id}:role/hms-dbmi-cnc-role</defaultValue>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>dataset_s3_object_key</name>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>destigmatized_dataset_s3_object_key</name>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>genomic_dataset_s3_object_key</name>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>infrastructure_git_hash</name>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.BooleanParameterDefinition>
          <name>isDestroyOnly</name>
          <defaultValue>false</defaultValue>
        </hudson.model.BooleanParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
  </properties>
  <scm class="hudson.plugins.git.GitSCM" plugin="git@5.2.1">
    <configVersion>2</configVersion>
    <userRemoteConfigs>
      <hudson.plugins.git.UserRemoteConfig>
        <url>${infrastructure_git_repo}</url>
      </hudson.plugins.git.UserRemoteConfig>
    </userRemoteConfigs>
    <branches>
      <hudson.plugins.git.BranchSpec>
        <name>${infrastructure_git_hash}</name>
      </hudson.plugins.git.BranchSpec>
    </branches>
    <doGenerateSubmoduleConfigurations>false</doGenerateSubmoduleConfigurations>
    <submoduleCfg class="empty-list"/>
    <extensions/>
  </scm>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <jdk>(System)</jdk>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command># json tag schema.  Used to uniquely identify a projects staging instances.
cat &lt;&lt;EOF &gt; staging_httpd_tags_file.json
[
  {
    &quot;Name&quot;: &quot;tag:Stack&quot;,
    &quot;Values&quot;: [&quot;$target_stack&quot;]
  },
  {
    &quot;Name&quot;: &quot;tag:Node&quot;,
    &quot;Values&quot;: [&quot;HTTPD&quot;]
  },
  {
    &quot;Name&quot;: &quot;tag:Project&quot;,
    &quot;Values&quot;: [&quot;$env_project&quot;]
  }
]
EOF

</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>#!/bin/bash
set -e
# Source folder containing the scripts
source_scripts_folder=&quot;${JENKINS_HOME}/workspace/Bash_Functions/&quot;
ls -la &quot;$source_scripts_folder&quot;

# Iterate through the files in the folder and source them
for script_file in &quot;$source_scripts_folder&quot;*.sh; do
    chmod +x &quot;$script_file&quot;
    if [ -f &quot;$script_file&quot; ] &amp;&amp; [ -x &quot;$script_file&quot; ]; then
        echo &quot;sourcing $script_file&quot;
        source &quot;$script_file&quot;
    fi
done

assume_role
# Deregister previous staging from staging TG (remove green from green)
target_group_vpc=$(get_target_group_vpc_by_tg_name $staging_tg_name)
staging_target_group_arn=$(get_target_group_arn_by_name &quot;$staging_tg_name&quot;)
staging_httpd_instance_prv_ips=($(get_private_ip_by_tags &quot;staging_httpd_tags_file.json&quot;)) # json generated in previous build step.

if [ -z &quot;${staging_httpd_instance_prv_ips}&quot; ]; then
   echo &quot;No private IPs found. No Staging Ips to deregister&quot;
else
   echo &quot;Deregistering staging target(s) before teardown.&quot;
   deregister_targets &quot;$target_group_vpc&quot; &quot;$staging_target_group_arn&quot; &quot;${staging_httpd_instance_prv_ips[@]}&quot;
   wait_for_target_group_health &quot;$staging_target_group_arn&quot; &quot;UNUSED&quot; &quot;${staging_httpd_instance_prv_ips[@]}&quot;
fi

reset_role</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>#!/bin/bash
set -e
# Source folder containing the scripts
source_scripts_folder=&quot;${JENKINS_HOME}/workspace/Bash_Functions/&quot;
ls -la &quot;$source_scripts_folder&quot;

# Iterate through the files in the folder and source them
for script_file in &quot;$source_scripts_folder&quot;*.sh; do
    chmod +x &quot;$script_file&quot;
    if [ -f &quot;$script_file&quot; ] &amp;&amp; [ -x &quot;$script_file&quot; ]; then
        echo &quot;sourcing $script_file&quot;
        source &quot;$script_file&quot;
    fi
done

# Destroy and Build resources
cd app-infrastructure

# TODO - Needs backend state management.
aws s3 cp s3://$stack_s3_bucket/deployment_state_metadata/${target_stack}/terraform.tfstate . || echo &quot;bad state or doesnt exist, it will be created&quot;

# Assume deployment role
assume_role

echo &quot;Database Secret Name: $database_app_user_secret_name&quot;

# Getting a secret value by secret name
db_app_user_json=$(fetch_secret &quot;$database_app_user_secret_name&quot;)
echo &quot;Database JSON: $db_app_user_json&quot;

# dbuser - seems to be the root user
picsure_db_username=$(extract_field &quot;$db_app_user_json&quot; &quot;username&quot;)
picsure_db_password=$(extract_field &quot;$db_app_user_json&quot; &quot;password&quot;)
picsure_db_host=$database_host_address

# Check if any of the variables are empty
if [ -z &quot;$picsure_db_username&quot; ] || [ -z &quot;$picsure_db_password&quot; ] || [ -z &quot;$picsure_db_host&quot; ]; then
  echo &quot;Error: One or more required database variables are empty.&quot; &gt;&amp;2
  exit 1
fi

# Get introspection token from database.
initialize_shared_db_config &quot;${picsure_db_host%:3306}&quot; &quot;$picsure_db_username&quot; &quot;$picsure_db_password&quot;
picsure_token_introspection_token=$(get_token_by_uuid &quot;$application_id_for_base_query&quot;)
echo &quot;Introspection Token: $picsure_token_introspection_token&quot;
unset_shared_db_config

# The database password likely contains special characters that don&apos;t play well with XML.
# We have a utility to replace the special characters so its XML safe.
picsure_db_password=$(replace_xml_special_chars &quot;${picsure_db_password}&quot;)

formatted_referer_allowed_domains=$(generate_domain_regex &quot;$referer_allowed_domains&quot;)

picsure_resource_uuid=&quot;02e23f52-f354-4e8b-992c-d37c8b9ba140&quot;
if [ &quot;$env_is_open_access&quot; == &quot;true&quot; ]; then
	picsure_resource_uuid=&quot;70c837be-5ffc-11eb-ae93-0242ac130002&quot;
fi

terraform init
if $isDestroyOnly; then

  terraform destroy -auto-approve \
     -var=&quot;dataset_s3_object_key=${dataset_s3_object_key}&quot;  \
     -var=&quot;destigmatized_dataset_s3_object_key=${destigmatized_dataset_s3_object_key}&quot;  \
     -var=&quot;genomic_dataset_s3_object_key=${genomic_dataset_s3_object_key}&quot; \
     -var=&quot;target_stack=${target_stack}&quot; \
     -var=&quot;picsure_rds_snapshot_id=${picsure_rds_snapshot_id}&quot; \
     -var=&quot;stack_githash=`echo ${deployment_git_hash} |cut -c1-7`&quot; \
     -var=&quot;stack_githash_long=${deployment_git_hash}&quot; \
     -var=&quot;env_public_dns_name=${env_public_dns_name}&quot; \
     -var=&quot;env_public_dns_name_staging=${env_public_dns_name_staging}&quot; \
     -var=&quot;env_private_dns_name=${env_private_dns_name}&quot; \
     -var=&quot;env_hosted_zone_id=${env_hosted_zone_id}&quot; \
     -var=&quot;analytics_id=${analytics_id}&quot; \
     -var=&quot;tag_manager_id=${tag_manager_id}&quot; \
     -var=&quot;env_is_open_access=${env_is_open_access}&quot; \
     -var=&quot;include_auth_hpds=${include_auth_hpds}&quot; \
     -var=&quot;include_open_hpds=${include_open_hpds}&quot; \
     -var=&quot;environment_name=${environment_name}&quot; \
     -var=&quot;env_staging_subdomain=${env_staging_subdomain}&quot; \
     -var=&quot;application_id_for_base_query=${application_id_for_base_query}&quot; \
     -var=&quot;stack_s3_bucket=${stack_s3_bucket}&quot; \
     -var=&quot;idp_provider=${idp_provider}&quot; \
     -var=&quot;env_project=${env_project}&quot; \
     -var=&quot;picsure_token_introspection_token=${picsure_token_introspection_token}&quot; \
     -var=&quot;fence_client_id=${fence_client_id}&quot; \
     -var=&quot;idp_provider_uri=${idp_provider_uri}&quot; \
     -var=&quot;client_id=${client_id}&quot; \
     -var=&quot;login_link=${login_link}&quot; \
     -var=&quot;connection_id=${connection_id}&quot; \
     -var=&quot;connection_label=${connection_label}&quot; \
     -var=&quot;connection_sub_prefix=${connection_sub_prefix}&quot; \
     -var=&quot;pdf_link=${pdf_link}&quot; \
     -var=&quot;help_link=${help_link}&quot; \
     -var=&quot;environment_prefix=${environment_prefix}&quot; \
     -var=&quot;referer_allowed_domains=${formatted_referer_allowed_domains}&quot; \
     -var=&quot;picsure_db_host=${picsure_db_host}&quot; \
     -var=&quot;pic_sure_resource_id=${picsure_resource_uuid}&quot; \
     -var=&quot;app_acct_id=${app_acct_id}&quot; \
     -var=&quot;app_user_secret_name=${database_app_user_secret_name}&quot; \
     -var=&quot;app_psql_user_secret_name=${database_psql_app_user_secret_name}&quot; || true


  reset_role

  # Move to terraform backend
  aws s3 cp terraform.tfstate s3://$stack_s3_bucket/deployment_state_metadata/${target_stack}/terraform.tfstate

else
  terraform destroy -auto-approve \
     -var=&quot;dataset_s3_object_key=${dataset_s3_object_key}&quot;  \
     -var=&quot;destigmatized_dataset_s3_object_key=${destigmatized_dataset_s3_object_key}&quot;  \
     -var=&quot;genomic_dataset_s3_object_key=${genomic_dataset_s3_object_key}&quot; \
     -var=&quot;target_stack=${target_stack}&quot; \
     -var=&quot;picsure_rds_snapshot_id=${picsure_rds_snapshot_id}&quot; \
     -var=&quot;stack_githash=`echo ${deployment_git_hash} |cut -c1-7`&quot; \
     -var=&quot;stack_githash_long=${deployment_git_hash}&quot; \
     -var=&quot;env_public_dns_name=${env_public_dns_name}&quot; \
     -var=&quot;env_public_dns_name_staging=${env_public_dns_name_staging}&quot; \
     -var=&quot;env_private_dns_name=${env_private_dns_name}&quot; \
     -var=&quot;env_hosted_zone_id=${env_hosted_zone_id}&quot; \
     -var=&quot;analytics_id=${analytics_id}&quot; \
     -var=&quot;tag_manager_id=${tag_manager_id}&quot; \
     -var=&quot;env_is_open_access=${env_is_open_access}&quot; \
     -var=&quot;include_auth_hpds=${include_auth_hpds}&quot; \
     -var=&quot;include_open_hpds=${include_open_hpds}&quot; \
     -var=&quot;environment_name=${environment_name}&quot; \
     -var=&quot;env_staging_subdomain=${env_staging_subdomain}&quot; \
     -var=&quot;application_id_for_base_query=${application_id_for_base_query}&quot; \
     -var=&quot;stack_s3_bucket=${stack_s3_bucket}&quot; \
     -var=&quot;idp_provider=${idp_provider}&quot; \
     -var=&quot;env_project=${env_project}&quot; \
     -var=&quot;picsure_token_introspection_token=${picsure_token_introspection_token}&quot; \
     -var=&quot;fence_client_id=${fence_client_id}&quot; \
     -var=&quot;idp_provider_uri=${idp_provider_uri}&quot; \
     -var=&quot;client_id=${client_id}&quot; \
     -var=&quot;login_link=${login_link}&quot; \
     -var=&quot;connection_id=${connection_id}&quot; \
     -var=&quot;connection_label=${connection_label}&quot; \
     -var=&quot;connection_sub_prefix=${connection_sub_prefix}&quot; \
     -var=&quot;pdf_link=${pdf_link}&quot; \
     -var=&quot;help_link=${help_link}&quot; \
     -var=&quot;environment_prefix=${environment_prefix}&quot; \
     -var=&quot;referer_allowed_domains=${formatted_referer_allowed_domains}&quot; \
     -var=&quot;picsure_db_host=${picsure_db_host}&quot; \
     -var=&quot;pic_sure_resource_id=${picsure_resource_uuid}&quot; \
     -var=&quot;app_acct_id=${app_acct_id}&quot; \
     -var=&quot;app_user_secret_name=${database_app_user_secret_name}&quot; \
     -var=&quot;app_psql_user_secret_name=${database_psql_app_user_secret_name}&quot; || true


  terraform apply -auto-approve \
     -var=&quot;dataset_s3_object_key=${dataset_s3_object_key}&quot;  \
     -var=&quot;destigmatized_dataset_s3_object_key=${destigmatized_dataset_s3_object_key}&quot;  \
     -var=&quot;genomic_dataset_s3_object_key=${genomic_dataset_s3_object_key}&quot; \
     -var=&quot;target_stack=${target_stack}&quot; \
     -var=&quot;picsure_rds_snapshot_id=${picsure_rds_snapshot_id}&quot; \
     -var=&quot;stack_githash=`echo ${deployment_git_hash} |cut -c1-7`&quot; \
     -var=&quot;stack_githash_long=${deployment_git_hash}&quot; \
     -var=&quot;env_public_dns_name=${env_public_dns_name}&quot; \
     -var=&quot;env_public_dns_name_staging=${env_public_dns_name_staging}&quot; \
     -var=&quot;env_private_dns_name=${env_private_dns_name}&quot; \
     -var=&quot;env_hosted_zone_id=${env_hosted_zone_id}&quot; \
     -var=&quot;analytics_id=${analytics_id}&quot; \
     -var=&quot;tag_manager_id=${tag_manager_id}&quot; \
     -var=&quot;env_is_open_access=${env_is_open_access}&quot; \
     -var=&quot;include_auth_hpds=${include_auth_hpds}&quot; \
     -var=&quot;include_open_hpds=${include_open_hpds}&quot; \
     -var=&quot;environment_name=${environment_name}&quot; \
     -var=&quot;env_staging_subdomain=${env_staging_subdomain}&quot; \
     -var=&quot;application_id_for_base_query=${application_id_for_base_query}&quot; \
     -var=&quot;stack_s3_bucket=${stack_s3_bucket}&quot; \
     -var=&quot;idp_provider=${idp_provider}&quot; \
     -var=&quot;env_project=${env_project}&quot; \
     -var=&quot;picsure_token_introspection_token=${picsure_token_introspection_token}&quot; \
     -var=&quot;fence_client_id=${fence_client_id}&quot; \
     -var=&quot;idp_provider_uri=${idp_provider_uri}&quot; \
     -var=&quot;client_id=${client_id}&quot; \
     -var=&quot;login_link=${login_link}&quot; \
     -var=&quot;connection_id=${connection_id}&quot; \
     -var=&quot;connection_label=${connection_label}&quot; \
     -var=&quot;connection_sub_prefix=${connection_sub_prefix}&quot; \
     -var=&quot;pdf_link=${pdf_link}&quot; \
     -var=&quot;help_link=${help_link}&quot; \
     -var=&quot;environment_prefix=${environment_prefix}&quot; \
     -var=&quot;referer_allowed_domains=${formatted_referer_allowed_domains}&quot; \
     -var=&quot;picsure_db_host=${picsure_db_host}&quot; \
     -var=&quot;pic_sure_resource_id=${picsure_resource_uuid}&quot; \
     -var=&quot;app_acct_id=${app_acct_id}&quot; \
     -var=&quot;app_user_secret_name=${database_app_user_secret_name}&quot; \
     -var=&quot;app_psql_user_secret_name=${database_psql_app_user_secret_name}&quot; || true


  reset_role

  # Move to terraform backend
  # If script fails before state file is uploaded to s3 state will be lost for created objects \
  # terraform destroy will not destroy anything as it points to old state and apply will fail as objects may already exist
  # Will have to manually destroy and delete roles, ec2s, etc. in that scenario
  aws s3 cp terraform.tfstate s3://$stack_s3_bucket/deployment_state_metadata/${target_stack}/terraform.tfstate
  # roles are now in the same state as their related resources..
  #aws s3 cp ../s3-deployment-roles/terraform.tfstate s3://$stack_s3_bucket/deployment_state_metadata/${target_stack}/terraform.tfstate_roles

  # Files that are needed, but are do not need to rendered by terraform.
  aws s3 --sse=AES256 cp configs/banner_config.json s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/banner_config.json
  aws s3 --sse=AES256 cp scripts/wildfly-docker.sh s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/wildfly-docker.sh
  aws s3 --sse=AES256 cp scripts/psama-docker.sh s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/psama-docker.sh
  aws s3 --sse=AES256 cp scripts/dictionary-docker.sh s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/dictionary-docker.sh  
  aws s3 --sse=AES256 cp scripts/httpd-docker.sh s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/httpd-docker.sh  

  # These files are uploaded to s3 because user-scripts download them.  The user scripts are already running
  # so this is a race condition at this point as terraform has been applied
  #  have terraform provision these to the ec2 and remove the aws cli stuff from the user-scripts and here
  aws s3 --sse=AES256 cp picsureui-settings.json s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/picsureui_settings.json
  aws s3 --sse=AES256 cp standalone.xml s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/standalone.xml
  aws s3 --sse=AES256 cp httpd-vhosts.conf s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/httpd-vhosts.conf
  aws s3 --sse=AES256 cp aggregate-resource.properties s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/aggregate-resource.properties
  aws s3 --sse=AES256 cp visualization-resource.properties s3://$stack_s3_bucket/configs/jenkins_pipeline_build_${deployment_git_hash}/visualization-resource.properties
fi</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers>
    <hudson.plugins.ws__cleanup.PreBuildCleanup plugin="ws-cleanup@0.46">
      <deleteDirs>false</deleteDirs>
      <cleanupParameter></cleanupParameter>
      <externalDelete></externalDelete>
      <disableDeferredWipeout>false</disableDeferredWipeout>
    </hudson.plugins.ws__cleanup.PreBuildCleanup>
  </buildWrappers>
</project>
